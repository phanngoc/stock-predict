{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import pad\n",
    "from underthesea import classify, word_tokenize, sent_tokenize\n",
    "from pickle import load\n",
    "\n",
    "\n",
    "max_len = 400\n",
    "\n",
    "def extract_hidden_state(outputs):\n",
    "    hiddenSize = outputs[0].size()[0]\n",
    "    # flatten dimension 1 and 2, and hold batch dimension 0\n",
    "    embeddings = outputs[0].view(hiddenSize, -1).numpy()\n",
    "    return embeddings\n",
    "\n",
    "def vectorize_mean(sentences):\n",
    "    encodedText = tokenizer.batch_encode_plus(\n",
    "        sentences, padding=True, truncation=True, max_length=300)['input_ids']\n",
    "    embeddedTitle = phobert(torch.tensor(encodedText))\n",
    "    with torch.no_grad():\n",
    "        print('hiddenState', extract_hidden_state(embeddedTitle).shape)\n",
    "\n",
    "    embedSqueeze = embeddedTitle['last_hidden_state'].squeeze(0)\n",
    "    print('embedSqueeze', embedSqueeze.shape)\n",
    "    embedMean = embedSqueeze.mean(axis=1)\n",
    "    return embedMean.detach().numpy()\n",
    "\n",
    "def trim_words(sentence, num_word=200):\n",
    "    # Split the sentence into words\n",
    "    words = sentence.split()\n",
    "    # Trim the sentence to 5 words\n",
    "    trimmed_sentence = \" \".join(words[:num_word])\n",
    "    return trimmed_sentence\n",
    "\n",
    "\n",
    "\n",
    "X_train = vectorize_mean(train_df_check['text'])\n",
    "y_train = train_df_check['labels'].values.tolist()\n",
    "print('y_train', np.array(y_train).shape, X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 2: Create and fit the SVR model\n",
    "model = SVR(kernel='linear')\n",
    "model.fit(x_train, y_train.ravel())  # ravel y_train to convert to 1D array\n",
    "\n",
    "# Step 5: Evaluate the model's performance (optional)\n",
    "y_pred = model.predict(x_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
