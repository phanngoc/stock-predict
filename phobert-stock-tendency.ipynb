{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/37kt02914kx36yzcbbqfyck00000gn/T/ipykernel_6132/2801794870.py:15: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import mysql.connector\n",
    "\n",
    "# Establish a connection to the MySQL database\n",
    "connection = mysql.connector.connect(\n",
    "    host='127.0.0.1',\n",
    "    port=13306,\n",
    "    user='root',\n",
    "    password='root',\n",
    "    database='pyml'\n",
    ")\n",
    "\n",
    "# Read the table data using pandas\n",
    "query = \"SELECT title, content, date FROM crawl_data where domain = 'https://vneconomy.vn/kinh-te-the-gioi.htm'\"\n",
    "df = pd.read_sql(query, connection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 xe điện có tầm đi dài nhất năm 2023</td>\n",
       "      <td>\\nNỗi quan ngại về tầm đi (range - quãng đường...</td>\n",
       "      <td>2023-06-08 10:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cử nhân thất nghiệp: “Vết sẹo” kinh tế Trung Quốc</td>\n",
       "      <td>\\nNhiều người trong số họ chấp nhận công việc ...</td>\n",
       "      <td>2023-06-09 07:11:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Giá vàng thế giới tăng mạnh nhờ đồng USD giảm ...</td>\n",
       "      <td>\\nGiá vàng thế giới tăng mạnh trong phiên giao...</td>\n",
       "      <td>2023-06-09 10:31:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20 thành phố đắt đỏ nhất thế giới với người nư...</td>\n",
       "      <td>\\nTheo Xếp hạng Chi phí Sinh hoạt năm 2023 của...</td>\n",
       "      <td>2023-06-08 10:22:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S&amp;P 500 đóng cửa cao nhất từ đầu năm, dầu sụt ...</td>\n",
       "      <td>\\nChứng khoán Mỹ tăng điểm trong phiên giao dị...</td>\n",
       "      <td>2023-06-09 07:56:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0             10 xe điện có tầm đi dài nhất năm 2023   \n",
       "1  Cử nhân thất nghiệp: “Vết sẹo” kinh tế Trung Quốc   \n",
       "2  Giá vàng thế giới tăng mạnh nhờ đồng USD giảm ...   \n",
       "3  20 thành phố đắt đỏ nhất thế giới với người nư...   \n",
       "4  S&P 500 đóng cửa cao nhất từ đầu năm, dầu sụt ...   \n",
       "\n",
       "                                             content                date  \n",
       "0  \\nNỗi quan ngại về tầm đi (range - quãng đường... 2023-06-08 10:57:00  \n",
       "1  \\nNhiều người trong số họ chấp nhận công việc ... 2023-06-09 07:11:00  \n",
       "2  \\nGiá vàng thế giới tăng mạnh trong phiên giao... 2023-06-09 10:31:00  \n",
       "3  \\nTheo Xếp hạng Chi phí Sinh hoạt năm 2023 của... 2023-06-08 10:22:00  \n",
       "4  \\nChứng khoán Mỹ tăng điểm trong phiên giao dị... 2023-06-09 07:56:00  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Ngày  Lần cuối        Mở       Cao      Thấp       KL % Thay đổi\n",
      "0  09/06/2023  1,107.53  1,101.32  1,107.53  1,097.23  902.21K      0.56%\n",
      "1  08/06/2023  1,101.32  1,109.54  1,115.14  1,101.32    1.32M     -0.74%\n",
      "2  07/06/2023  1,109.54  1,108.31  1,112.28  1,104.26    1.00M      0.11%\n",
      "3  06/06/2023  1,108.31  1,097.82  1,108.31  1,097.82  842.76K      0.96%\n",
      "4  05/06/2023  1,097.82  1,090.84  1,103.81  1,090.84  948.36K      0.64%\n",
      "                                               title  \\\n",
      "0             10 xe điện có tầm đi dài nhất năm 2023   \n",
      "1  Cử nhân thất nghiệp: “Vết sẹo” kinh tế Trung Quốc   \n",
      "2  Giá vàng thế giới tăng mạnh nhờ đồng USD giảm ...   \n",
      "3  20 thành phố đắt đỏ nhất thế giới với người nư...   \n",
      "4  S&P 500 đóng cửa cao nhất từ đầu năm, dầu sụt ...   \n",
      "\n",
      "                                             content        date  \n",
      "0  \\nNỗi quan ngại về tầm đi (range - quãng đường...  08/06/2023  \n",
      "1  \\nNhiều người trong số họ chấp nhận công việc ...  09/06/2023  \n",
      "2  \\nGiá vàng thế giới tăng mạnh trong phiên giao...  09/06/2023  \n",
      "3  \\nTheo Xếp hạng Chi phí Sinh hoạt năm 2023 của...  08/06/2023  \n",
      "4  \\nChứng khoán Mỹ tăng điểm trong phiên giao dị...  09/06/2023  \n"
     ]
    }
   ],
   "source": [
    "dfIndex = pd.read_csv('./data/vn_index.csv')\n",
    "print(dfIndex.head())\n",
    "\n",
    "# how to format date from '2023-06-08 10:57:00' to '09/06/2023' ?\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%d/%m/%Y')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0             10 xe điện có tầm đi dài nhất năm 2023   \n",
      "1  Cử nhân thất nghiệp: “Vết sẹo” kinh tế Trung Quốc   \n",
      "2  Giá vàng thế giới tăng mạnh nhờ đồng USD giảm ...   \n",
      "3  20 thành phố đắt đỏ nhất thế giới với người nư...   \n",
      "4  S&P 500 đóng cửa cao nhất từ đầu năm, dầu sụt ...   \n",
      "\n",
      "                                             content        date        Ngày  \\\n",
      "0  \\nNỗi quan ngại về tầm đi (range - quãng đường...  08/06/2023  08/06/2023   \n",
      "1  \\nNhiều người trong số họ chấp nhận công việc ...  09/06/2023  09/06/2023   \n",
      "2  \\nGiá vàng thế giới tăng mạnh trong phiên giao...  09/06/2023  09/06/2023   \n",
      "3  \\nTheo Xếp hạng Chi phí Sinh hoạt năm 2023 của...  08/06/2023  08/06/2023   \n",
      "4  \\nChứng khoán Mỹ tăng điểm trong phiên giao dị...  09/06/2023  09/06/2023   \n",
      "\n",
      "   Lần cuối        Mở       Cao      Thấp       KL % Thay đổi  \n",
      "0  1,101.32  1,109.54  1,115.14  1,101.32    1.32M     -0.74%  \n",
      "1  1,107.53  1,101.32  1,107.53  1,097.23  902.21K      0.56%  \n",
      "2  1,107.53  1,101.32  1,107.53  1,097.23  902.21K      0.56%  \n",
      "3  1,101.32  1,109.54  1,115.14  1,101.32    1.32M     -0.74%  \n",
      "4  1,107.53  1,101.32  1,107.53  1,097.23  902.21K      0.56%  \n"
     ]
    }
   ],
   "source": [
    "# how to merge df and dfIndex by date and another column ?\n",
    "\n",
    "dfMerge = pd.merge(df, dfIndex, left_on=['date'], right_on=['Ngày'], how='left')\n",
    "print(dfMerge.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 title  \\\n",
      "741  Tiền ảo 2023: Không chỉ là “mùa đông”, thậm ch...   \n",
      "627  Giá vàng trong nước tiếp tục trượt dốc, thế gi...   \n",
      "629  Các nhà đầu tư lớn “rón rén” quay trở lại thị ...   \n",
      "630  Thế khó của Chủ tịch Fed trong cuộc họp báo ng...   \n",
      "631  Đặt cược sai vào cổ phiếu ô tô điện, Amazon hứ...   \n",
      "\n",
      "                                               content        date  \\\n",
      "741  \\nTrong thế giới tiền ảo, các nhà đầu tư đã qu...  01/01/2023   \n",
      "627  \\nGiá vàng miếng trong nước sáng nay (1/2) tiế...  01/02/2023   \n",
      "629  \\nHãng tin Reuters dẫn số liệu từ công ty quản...  01/02/2023   \n",
      "630  \\nCuộc họp chính sách tiền tệ đầu tiên trong n...  01/02/2023   \n",
      "631  \\nTrong 9 tháng đầu năm 2022, Amazon ghi nhận ...  01/02/2023   \n",
      "\n",
      "           Ngày  Lần cuối        Mở       Cao      Thấp     KL % Thay đổi  \n",
      "741         NaN       NaN       NaN       NaN       NaN    NaN        NaN  \n",
      "627  01/02/2023  1,075.97  1,111.18  1,116.96  1,075.97  1.03M     -3.17%  \n",
      "629  01/02/2023  1,075.97  1,111.18  1,116.96  1,075.97  1.03M     -3.17%  \n",
      "630  01/02/2023  1,075.97  1,111.18  1,116.96  1,075.97  1.03M     -3.17%  \n",
      "631  01/02/2023  1,075.97  1,111.18  1,116.96  1,075.97  1.03M     -3.17%  \n"
     ]
    }
   ],
   "source": [
    "df_sorted = dfMerge.sort_values(by='date', ascending=True)\n",
    "print(df_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741    -0.000291\n",
      "627    -0.031700\n",
      "629    -0.031700\n",
      "630    -0.031700\n",
      "631    -0.031700\n",
      "637    -0.031700\n",
      "620    -0.031700\n",
      "626    -0.031700\n",
      "621    -0.031700\n",
      "628    -0.031700\n",
      "490     0.015500\n",
      "494     0.015500\n",
      "489     0.015500\n",
      "330    -0.000291\n",
      "329    -0.000291\n",
      "331    -0.000291\n",
      "197    -0.000291\n",
      "201    -0.000291\n",
      "35      0.003000\n",
      "41      0.003000\n",
      "40      0.003000\n",
      "38      0.003000\n",
      "39      0.003000\n",
      "36      0.003000\n",
      "37      0.003000\n",
      "1378   -0.000291\n",
      "1377   -0.000291\n",
      "1376   -0.000291\n",
      "1374   -0.000291\n",
      "1368   -0.000291\n",
      "1370   -0.000291\n",
      "1369   -0.000291\n",
      "1372   -0.000291\n",
      "1194   -0.000291\n",
      "1198   -0.000291\n",
      "1203   -0.000291\n",
      "1032    0.005700\n",
      "1033    0.005700\n",
      "1034    0.005700\n",
      "1035    0.005700\n",
      "1037    0.005700\n",
      "1038    0.005700\n",
      "1042    0.005700\n",
      "1036    0.005700\n",
      "865    -0.011600\n",
      "864    -0.011600\n",
      "866    -0.011600\n",
      "863    -0.011600\n",
      "868    -0.011600\n",
      "869    -0.011600\n",
      "Name: change_percent, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import math\n",
    "import numpy as np\n",
    "# Create an instance of SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Extract the column with missing values from the DataFrame\n",
    "def convertToFloat(percentage_str):\n",
    "    if type(percentage_str) == str:\n",
    "        percentage_str = percentage_str.strip('%')\n",
    "        # Convert to decimal.\n",
    "        decimal_value = float(percentage_str) / 100\n",
    "        return decimal_value\n",
    "    else:\n",
    "        return percentage_str\n",
    "\n",
    "columnDiffValue = df_sorted['% Thay đổi'].apply(convertToFloat)\n",
    "\n",
    "# Reshape the column to match the input requirements of SimpleImputer\n",
    "column_reshaped = columnDiffValue.values.reshape(-1, 1)\n",
    "\n",
    "# Fit the imputer on the column\n",
    "imputer.fit(column_reshaped)\n",
    "\n",
    "# Transform the column by filling missing values\n",
    "column_filled = imputer.transform(column_reshaped)\n",
    "\n",
    "# Replace the original column with the imputed values\n",
    "df_sorted['change_percent'] = column_filled\n",
    "print(df_sorted['change_percent'][:50])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a histogram of the column\n",
    "plt.hist(df_sorted[\"change_percent\"])\n",
    "\n",
    "# Show the histogram\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted['content'] = df_sorted['content'].str[:60]\n",
    "df_sorted.to_csv('./data/vn_index_merge_content.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents: ['\\nNỗi quan ngại về tầm đi (range - quãng đường đi được mỗi lần sạc đầy pin) là một trong những lý do lớn nhất khiến người dùng băn khoăn khi chuyển sang sử dụng xe điện.\\xa0\\nNăm 2021, tầm đi\\xa0 bình quân của xe điện tại Mỹ chỉ là 349 km, thấp hơn đáng kể so với mức bình quân của xe chạy xăng là 665 km. Tuy nhiên, theo biểu đồ dưới đây, tầm đi của xe điện đã tăng lên đáng kể với mức 480 km mỗi lần sạc đầy ngày càng phổ biến\\nDưới đây là top 10 xe điện có quãng đường di chuyển dài nhất mỗi lần sạc đầy năm 2023 tại thị trường Mỹ.\\xa0\\n\\n\\n\\n']\n",
      "len: 1498\n",
      "documents: (400, 768)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# (1, 239, 768)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdocuments:\u001b[39m\u001b[39m'\u001b[39m, vectorize(text)\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 53\u001b[0m X_train \u001b[39m=\u001b[39m vectorize_arr(documents)\n",
      "Cell \u001b[0;32mIn[56], line 41\u001b[0m, in \u001b[0;36mvectorize_arr\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvectorize_arr\u001b[39m(sentences):\n\u001b[0;32m---> 41\u001b[0m     t \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([vectorize(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences])\n\u001b[1;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "Cell \u001b[0;32mIn[56], line 41\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvectorize_arr\u001b[39m(sentences):\n\u001b[0;32m---> 41\u001b[0m     t \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([vectorize(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences])\n\u001b[1;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "Cell \u001b[0;32mIn[56], line 34\u001b[0m, in \u001b[0;36mvectorize\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     32\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([tokenizer\u001b[39m.\u001b[39mencode(sentence_token)])\n\u001b[1;32m     33\u001b[0m input_ids \u001b[39m=\u001b[39m trim_tensor(input_ids, \u001b[39m256\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m features \u001b[39m=\u001b[39m phobert(input_ids)  \u001b[39m# Models outputs are now tuples\u001b[39;00m\n\u001b[1;32m     35\u001b[0m features_np \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mlast_hidden_state\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     36\u001b[0m padding_length \u001b[39m=\u001b[39m max_len \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(features_np[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/adapters/context.py:108\u001b[0m, in \u001b[0;36mForwardContext.wrap.<locals>.wrapper_func\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m ctx:\n\u001b[1;32m    105\u001b[0m     kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    106\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39moutput_\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcontext_attributes\n\u001b[1;32m    107\u001b[0m     }\n\u001b[0;32m--> 108\u001b[0m     results \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    110\u001b[0m     \u001b[39m# append output attributes\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:883\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    874\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    875\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    876\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    879\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minvertible_adapters_forward(embedding_output)\n\u001b[0;32m--> 883\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    884\u001b[0m     embedding_output,\n\u001b[1;32m    885\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    886\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    887\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    888\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    889\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    890\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    891\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    892\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    893\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    894\u001b[0m )\n\u001b[1;32m    895\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    896\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:551\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    542\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    543\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    544\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    548\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    549\u001b[0m     )\n\u001b[1;32m    550\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 551\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    552\u001b[0m         hidden_states,\n\u001b[1;32m    553\u001b[0m         attention_mask,\n\u001b[1;32m    554\u001b[0m         layer_head_mask,\n\u001b[1;32m    555\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    556\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    557\u001b[0m         past_key_value,\n\u001b[1;32m    558\u001b[0m         output_attentions,\n\u001b[1;32m    559\u001b[0m     )\n\u001b[1;32m    561\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    562\u001b[0m (attention_mask,) \u001b[39m=\u001b[39m adjust_tensors_for_parallel(hidden_states, attention_mask)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:436\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    425\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    426\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    434\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    437\u001b[0m         hidden_states,\n\u001b[1;32m    438\u001b[0m         attention_mask,\n\u001b[1;32m    439\u001b[0m         head_mask,\n\u001b[1;32m    440\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    441\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    443\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    445\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:360\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    351\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    352\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    359\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 360\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    361\u001b[0m         hidden_states,\n\u001b[1;32m    362\u001b[0m         attention_mask,\n\u001b[1;32m    363\u001b[0m         head_mask,\n\u001b[1;32m    364\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    365\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    366\u001b[0m         past_key_value,\n\u001b[1;32m    367\u001b[0m         output_attentions,\n\u001b[1;32m    368\u001b[0m     )\n\u001b[1;32m    369\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    370\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:248\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    243\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    245\u001b[0m key_layer, value_layer, attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefix_tuning(\n\u001b[1;32m    246\u001b[0m     key_layer, value_layer, hidden_states, attention_mask\n\u001b[1;32m    247\u001b[0m )\n\u001b[0;32m--> 248\u001b[0m (query_layer,) \u001b[39m=\u001b[39m adjust_tensors_for_parallel(key_layer, query_layer)\n\u001b[1;32m    250\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(query_layer, key_layer\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/adapters/composition.py:208\u001b[0m, in \u001b[0;36madjust_tensors_for_parallel\u001b[0;34m(hidden_states, *tensors)\u001b[0m\n\u001b[1;32m    206\u001b[0m     repeats \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(tensor\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    207\u001b[0m     repeats[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 208\u001b[0m     new_tensor \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39;49mrepeat(\u001b[39m*\u001b[39;49mrepeats)\n\u001b[1;32m    209\u001b[0m     outputs\u001b[39m.\u001b[39mappend(new_tensor)\n\u001b[1;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import re\n",
    "from underthesea import word_tokenize, sent_tokenize\n",
    "from gensim import corpora, models\n",
    "import torch\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower() # lowercase text\n",
    "    return text\n",
    "\n",
    "documents = df['content'].values.tolist()\n",
    "print('documents:', documents[0:1])\n",
    "print('len:', len(documents))\n",
    "\n",
    "def trim_tensor(tensor, max_length):\n",
    "    if tensor.size(1) > max_length:\n",
    "        tensor = tensor[:, :max_length]  # Trim the tensor to the max_length\n",
    "    return tensor\n",
    "\n",
    "from torch.nn.functional import pad\n",
    "def trim_words(sentence, num_word = 200):\n",
    "    # Split the sentence into words\n",
    "    words = sentence.split()\n",
    "    # Trim the sentence to 5 words\n",
    "    trimmed_sentence = \" \".join(words[:num_word])\n",
    "    return trimmed_sentence\n",
    "\n",
    "max_len = 400\n",
    "def vectorize(sentence):\n",
    "    sentence_token = trim_words(word_tokenize(sentence, format=\"text\"))\n",
    "    input_ids = torch.tensor([tokenizer.encode(sentence_token)])\n",
    "    input_ids = trim_tensor(input_ids, 256)\n",
    "    features = phobert(input_ids)  # Models outputs are now tuples\n",
    "    features_np = features.last_hidden_state.detach()\n",
    "    padding_length = max_len - len(features_np[0])\n",
    "    padded_a = pad(features_np, (0, 0, 0, padding_length), value=0, mode='constant')\n",
    "    return padded_a.detach().numpy()[0]\n",
    "\n",
    "def vectorize_arr(sentences):\n",
    "    t = np.array([vectorize(sentence) for sentence in sentences])\n",
    "    return t\n",
    "\n",
    "text = \"\"\"\n",
    "Chứng khoán Mỹ khởi sắc vào ngày thứ Ba (14/3), khi nhà đầu tư đặt cược vào việc nguy cơ lan truyền sau vụ phá sản của các ngân hàng Silicon Valley và Signature đã được ngăn chặn.\n",
    "Kết thúc phiên giao dịch ngày thứ Ba, chỉ số Dow Jones tăng 336.26 điểm (tương đương 1.06%) lên 32,155.40 điểm, chấm dứt chuỗi 5 phiên lao dốc liên tiếp. Chỉ số S&P 500 tiến 1.65% lên 3,919.29 điểm. Chỉ số Nasdaq Composite cộng 2.14% lên 11,428.15 điểm.\n",
    "Sự nhiệt tình mua vào các cổ phiếu ngân hàng của nhà đầu tư đã giảm đi phần nào vào buổi chiều. Tuy nhiên, nhiều cổ phiếu vẫn ghi nhận đà tăng, đánh dấu bước đảo chiều sau 2 phiên giảm sâu khi nhà đầu tư ngày càng tin tưởng rằng những ngân hàng đó sẽ không chịu chung số phận như ngân hàng Silicon Valley và Signature. Các nhà quản lý cho biết vào ngày 12/3 rằng họ đã xây dựng một kế hoạch để can thiệp cho tất cả những người gửi tiền ở 2 ngân hàng này.\n",
    "Chứng chỉ quỹ SPDR S&P Regional Banking ETF tiến 2%, phục hồi phần nào sau đà lao dốc 12% trong phiên trước đó. Cổ phiếu First Republic Bank bứt phá gần 27% sau khi bốc hơi gần 62% vào ngày 13/3. Cổ phiếu KeyCorp vọt gần 7% trong đợt phục hồi nhẹ sau khi trượt dốc 27%.\n",
    "\"\"\"\n",
    "\n",
    "# (1, 239, 768)\n",
    "print('documents:', vectorize(text).shape)\n",
    "X_train = vectorize_arr(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.000291]\n",
      " [-0.0317  ]\n",
      " [-0.0317  ]\n",
      " [-0.0317  ]\n",
      " [-0.0317  ]\n",
      " [-0.0317  ]\n",
      " [-0.0317  ]\n",
      " [-0.0317  ]\n",
      " [-0.0317  ]\n",
      " [-0.0317  ]\n",
      " [ 0.0155  ]\n",
      " [ 0.0155  ]\n",
      " [ 0.0155  ]\n",
      " [-0.000291]\n",
      " [-0.000291]\n",
      " [-0.000291]\n",
      " [-0.000291]\n",
      " [-0.000291]\n",
      " [ 0.003   ]\n",
      " [ 0.003   ]]\n"
     ]
    }
   ],
   "source": [
    "Y_train_pure = df_sorted['change_percent'].values.tolist()\n",
    "Y_train = np.expand_dims(Y_train_pure, axis = 1)\n",
    "print(Y_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 400, 200)         695200    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " layer_normalization_1 (Laye  (None, 400, 200)         400       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 400, 2)            402       \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 2)                0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 696,005\n",
      "Trainable params: 696,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, BatchNormalization, Dense, LSTM, Bidirectional, GlobalAveragePooling1D, Input, Average, LayerNormalization\n",
    "\n",
    "input_shape = (400, 768)\n",
    "model = Sequential()\n",
    "model.add(Input(shape=input_shape))\n",
    "model.add(Bidirectional(LSTM(\n",
    "        100, \n",
    "        return_sequences = True, \n",
    "        recurrent_dropout=0.2)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "150/150 [==============================] - 127s 830ms/step - loss: 5.5724e-04 - mae: 0.0107\n",
      "Epoch 2/15\n",
      "150/150 [==============================] - 248s 2s/step - loss: 1.7588e-04 - mae: 0.0088\n",
      "Epoch 3/15\n",
      "150/150 [==============================] - 311s 2s/step - loss: 1.7576e-04 - mae: 0.0089\n",
      "Epoch 4/15\n",
      "150/150 [==============================] - 272s 2s/step - loss: 1.7645e-04 - mae: 0.0089\n",
      "Epoch 5/15\n",
      "150/150 [==============================] - 273s 2s/step - loss: 1.7680e-04 - mae: 0.0089\n",
      "Epoch 6/15\n",
      "150/150 [==============================] - 239s 2s/step - loss: 1.7621e-04 - mae: 0.0090\n",
      "Epoch 7/15\n",
      "150/150 [==============================] - 240s 2s/step - loss: 1.7557e-04 - mae: 0.0089\n",
      "Epoch 8/15\n",
      "150/150 [==============================] - 253s 2s/step - loss: 1.7706e-04 - mae: 0.0089\n",
      "Epoch 9/15\n",
      "150/150 [==============================] - 264s 2s/step - loss: 1.7823e-04 - mae: 0.0090\n",
      "Epoch 10/15\n",
      "150/150 [==============================] - 274s 2s/step - loss: 1.7722e-04 - mae: 0.0089\n",
      "Epoch 11/15\n",
      "150/150 [==============================] - 275s 2s/step - loss: 1.7615e-04 - mae: 0.0089\n",
      "Epoch 12/15\n",
      "150/150 [==============================] - 264s 2s/step - loss: 1.7559e-04 - mae: 0.0089\n",
      "Epoch 13/15\n",
      "150/150 [==============================] - 265s 2s/step - loss: 1.7696e-04 - mae: 0.0089\n",
      "Epoch 14/15\n",
      "150/150 [==============================] - 1388s 9s/step - loss: 1.7714e-04 - mae: 0.0090\n",
      "Epoch 15/15\n",
      "150/150 [==============================] - 110s 735ms/step - loss: 1.7683e-04 - mae: 0.0090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x109349b10>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train, Y_train, epochs=15, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/stock_predict.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len sentence_token: 43\n",
      "trim_tensor 45\n",
      "trim_tensor after trim torch.Size([1, 45])\n",
      "len input_ids: torch.Size([1, 45])\n",
      "len sentence_token: 66\n",
      "trim_tensor 70\n",
      "trim_tensor after trim torch.Size([1, 70])\n",
      "len input_ids: torch.Size([1, 70])\n",
      "len sentence_token: 41\n",
      "trim_tensor 47\n",
      "trim_tensor after trim torch.Size([1, 47])\n",
      "len input_ids: torch.Size([1, 47])\n",
      "len sentence_token: 57\n",
      "trim_tensor 68\n",
      "trim_tensor after trim torch.Size([1, 68])\n",
      "len input_ids: torch.Size([1, 68])\n",
      "test_data_c: (4, 400, 768)\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "result:\n",
      " [[0.00038971]\n",
      " [0.00038971]\n",
      " [0.00038971]\n",
      " [0.00038971]]\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    \"Theo hãng tin Bloomberg, nhà sản xuất ở Trung Quốc - từ các công ty chuyên về đồ trang trí Giáng sinh cho tới các hãng quần áo và lều bạt - đều cho biết lượng đơn hàng mà họ nhận được từ khách hàng nước ngoài đang ngày càng giảm.\",\n",
    "    \"Giá vàng thế giới sụt mạnh về gần mốc chủ chốt 1.700 USD/oz, hoàn tất chuỗi tháng giảm dài nhất kể từ năm 2018 dưới sức ép từ chiến dịch tăng lãi suất của các ngân hàng trung ương trên toàn cầu. Giá vàng miếng trong nước sáng nay (1/9) không giảm, dẫn tới chênh lệch giá vàng trong nước-thế giới tăng lên 18 triệu đồng/lượng.\",\n",
    "    \"Cách đây chưa lâu, các cổ phiếu công nghệ vốn hoá lớn - đặc biệt là nhóm FAANG (gồm Facebook, Apple, Amazon, Netflix và Goolge) - được giới đầu tư ở Phố Wall xem là “những ngôi sao sáng”\",\n",
    "    \"Nord Stream 1, đường ống dẫn khí đốt lớn nhất từ Nga tới châu Âu, sẽ không thể hoạt động trở lại chừng nào Siemens Energy còn chưa sửa xong thiết bị hỏng. Đó là tuyên bố được Phó tổng giám đốc của hãng khí đốt quốc doanh Nga Gazprom, ông Vitaly Markelov, đưa ra trong cuộc trả lời phỏng vấn độc quyền hãng tin Reuters vào ngày 6/9.\"\n",
    "]\n",
    "\n",
    "test_data_c = vectorize_arr(test_data)\n",
    "print('test_data_c:', test_data_c.shape)\n",
    "result = model.predict(test_data_c)\n",
    "print('result:\\n', result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
